# -*- coding: utf-8 -*-
"""Automated Weather .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fD92-7ngAmy54Mkp2TWRFvFR7WjtU6ai
"""

from google.colab import files
import zipfile

uploaded = files.upload()

for fn in uploaded.keys():
  with zipfile.ZipFile(fn, 'r') as zip_ref:
    zip_ref.extractall('./')

# Libraries
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras import backend as K
from sklearn.model_selection import train_test_split
import numpy as np
import os

# Set path to the dataset
data_path = '/content/Dataset'

# Set image size and batch size
img_width, img_height = 299, 299
batch_size = 32

# Load the InceptionV3 model without the top layers
base_model = InceptionV3(weights='imagenet', include_top=False)

# Add new top layers for the new classification task
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(4, activation='softmax')(x)  # Adjusted for 4 classes
model = Model(inputs=base_model.input, outputs=predictions)

# Freeze the original InceptionV3 layers
for layer in base_model.layers:
    layer.trainable = False

# Compile the model
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

# Prepare the data
X = []
Y = []
labels = ['cloudy', 'shine', 'rain', 'sunrise']
for i, label in enumerate(labels):
    folder_path = os.path.join(data_path, label)
    for img_name in os.listdir(folder_path):
        img_path = os.path.join(folder_path, img_name)
        img = image.load_img(img_path, target_size=(img_width, img_height))
        x = image.img_to_array(img)
        X.append(x)
        Y.append(i)
X = np.array(X)
Y = np.array(Y)
Y = np.eye(len(labels))[Y]

# Split the data into training, validation, and test sets
x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)
x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.1, random_state=42)

# Train the model
history = model.fit(x_train, y_train, batch_size=batch_size, epochs=10, validation_data=(x_val, y_val))

# Evaluate the model on the test set
loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)
print("Test loss:", loss)
print("Test accuracy:", acc)